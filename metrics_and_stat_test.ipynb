{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "753a6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy import stats\n",
    "from mlxtend.evaluate import mcnemar_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51784a3e",
   "metadata": {},
   "source": [
    "## functions to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://www.deepset.ai/blog/metrics-to-evaluate-a-question-answering-system\n",
    "# https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12a6ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall measures how many times the correct document was among the retrieved documents\n",
    "# For a single query, the output is binary: either a document is contained in the selection, or it is not\n",
    "def correct_retrieved_doc(row):\n",
    "    source_cols = [\"source_1\", \"source_2\", \"source_3\", \"source_4\", \"source_5\"]\n",
    "    for col in source_cols:\n",
    "        if row[col] == row[\"source\"]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def recall(output):\n",
    "    return sum(output.apply(correct_retrieved_doc)) / output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8825edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match measures the proportion of documents where the predicted answer is identical to the correct answer\n",
    "def is_exact_match(row):\n",
    "    if row[\"actual_answer\"] == row[\"answer\"]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def exact_match(output):\n",
    "    return sum(output.apply(is_exact_match)) / output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55d23fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 measures the word overlap between the labeled and the predicted answer\n",
    "# tp: number of tokens* that are shared between the correct answer and the prediction.\n",
    "# fp: number of tokens that are in the prediction but not in the correct answer.\n",
    "# fn: number of tokens that are in the correct answer but not in the prediction.\n",
    "def get_f1(row):\n",
    "    real_answer = row[\"actual_answer\"].split()\n",
    "    gen_answer = row[\"answer\"].split()\n",
    "    common = collections.Counter(real_answer) & collections.Counter(gen_answer)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(real_answer) == 0 or len(gen_answer) == 0:\n",
    "        return int(real_answer == gen_answer)\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(gen_answer)\n",
    "    recall = 1.0 * num_same / len(real_answer)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def overall_f1(output):\n",
    "    return sum(output.apply(get_f1)) / output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b865f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical significance\n",
    "# mcnemar test for recall and exact match\n",
    "def stat_sig_mcnemar(baseline, variation):\n",
    "    contingency_table = mcnemar_table(y_target=np.ones(len(baseline)), y_model1=baseline, y_model2=variation)\n",
    "    return mcnemar(contingency_table).pvalue\n",
    "\n",
    "# t test for f1\n",
    "def stat_sig_t_test(baseline, variation):\n",
    "    t_stat, p_val = stats.ttest_ind(baseline, variation)\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9a2e9",
   "metadata": {},
   "source": [
    "## load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63155ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rag_baseline = pd.read_csv(\"results/results_baseline.csv\")\n",
    "non_rag_baseline.drop([\"source_1\", \"source_2\", \"source_3\", \"source_4\", \"source_5\"], axis=1, inplace=True)\n",
    "rag_baseline = pd.read_csv(\"results/results_rag_bad_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7397308",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation1 = pd.read_csv(\"results/results_rag.csv\")\n",
    "variation2 = pd.read_csv(\"results/results_rag_few_shot_bad_embedding.csv\")\n",
    "variation3 = pd.read_csv(\"results/results_rag_few_shot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d0a6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"gemma3_questions_balanced_sample_200.csv\")\n",
    "train_data.drop([\"source\", \"gemma3:12b_answer\", \"gemma3:12b_question\"], axis=1, inplace=True)\n",
    "train_data.columns = [\"source\", \"question_type\", \"content_category\", \"actual_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, answer, source_1, source_2, source_3, source_4, source_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fa12eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results with train data\n",
    "non_rag_baseline = pd.concat([non_rag_baseline, train_data], axis=1)\n",
    "rag_baseline = pd.concat([rag_baseline, train_data], axis=1)\n",
    "variation1 = pd.concat([variation1, train_data], axis=1)\n",
    "variation2 = pd.concat([variation2, train_data], axis=1)\n",
    "variation3 = pd.concat([variation3, train_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0836e",
   "metadata": {},
   "source": [
    "## calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd7d68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics for baseline models\n",
    "non_rag_baseline[\"em\"] = non_rag_baseline.apply(is_exact_match, axis=1)\n",
    "non_rag_baseline[\"f1\"] = non_rag_baseline.apply(get_f1, axis=1)\n",
    "\n",
    "rag_baseline[\"recall\"] = rag_baseline.apply(correct_retrieved_doc, axis=1)\n",
    "rag_baseline[\"em\"] = rag_baseline.apply(is_exact_match, axis=1)\n",
    "rag_baseline[\"f1\"] = rag_baseline.apply(get_f1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "165a3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non rag metrics\n",
      "em: 0.0\n",
      "f1: 0.05685775923885378\n",
      "\n",
      "\n",
      "rag baseline metrics\n",
      "recall: 0.41\n",
      "em: 0.0\n",
      "f1: 0.11131313260860196\n"
     ]
    }
   ],
   "source": [
    "print(\"non rag metrics\")\n",
    "print(f'em: {non_rag_baseline[\"em\"].mean()}')\n",
    "print(f'f1: {non_rag_baseline[\"f1\"].mean()}')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"rag baseline metrics\")\n",
    "print(f'recall: {rag_baseline[\"recall\"].mean()}')\n",
    "print(f'em: {rag_baseline[\"em\"].mean()}')\n",
    "print(f'f1: {rag_baseline[\"f1\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cf0078ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non rag vs rag baseline\n",
      "em: 1.0\n",
      "f1: 2.5059085750187265e-06\n"
     ]
    }
   ],
   "source": [
    "# statistical test for em, f1 for non rag vs rag baseline\n",
    "print(\"non rag vs rag baseline\")\n",
    "print(\"em:\", stat_sig_mcnemar(non_rag_baseline[\"em\"], rag_baseline[\"em\"]))\n",
    "print(\"f1:\", stat_sig_t_test(non_rag_baseline[\"f1\"], rag_baseline[\"f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2c16ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variation 1 metrics\n",
      "recall: 0.545\n",
      "em: 0.0\n",
      "f1: 0.12425176189147388\n",
      "\n",
      "\n",
      "rag baseline vs variation 1\n",
      "recall: 4.1934157934520044e-05\n",
      "em: 1.0\n",
      "f1: 0.3479308473811954\n",
      "\n",
      "\n",
      "variation 2 metrics\n",
      "recall: 0.41\n",
      "em: 0.0\n",
      "f1: 0.11131313260860196\n",
      "\n",
      "\n",
      "rag baseline vs variation 2\n",
      "recall: 1.0\n",
      "em: 1.0\n",
      "f1: 1.0\n",
      "\n",
      "\n",
      "variation 3 metrics\n",
      "recall: 0.545\n",
      "em: 0.0\n",
      "f1: 0.13067888956981413\n",
      "\n",
      "\n",
      "rag baseline vs variation 3\n",
      "recall: 4.1934157934520044e-05\n",
      "em: 1.0\n",
      "f1: 0.19635264626893023\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variations = [variation1, variation2, variation3]\n",
    "var_names = [\"variation 1\", \"variation 2\", \"variation 3\"]\n",
    "metric_cols = [\"recall\", \"em\", \"f1\"]\n",
    "\n",
    "for i in range(len(variations)):\n",
    "    var = variations[i]\n",
    "    # add columns with metrics\n",
    "    var[\"recall\"] = var.apply(correct_retrieved_doc, axis=1)\n",
    "    var[\"em\"] = var.apply(is_exact_match, axis=1)\n",
    "    var[\"f1\"] = var.apply(get_f1, axis=1)\n",
    "    \n",
    "    # print metrics\n",
    "    print(f'{var_names[i]} metrics')\n",
    "    print(\"recall:\", var[\"recall\"].mean())\n",
    "    print(\"em:\", var[\"em\"].mean())\n",
    "    print(\"f1:\", var[\"f1\"].mean())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # significance tests against rag baseline\n",
    "    print(f'rag baseline vs {var_names[i]}')\n",
    "    print(\"recall:\", stat_sig_mcnemar(rag_baseline[\"recall\"], var[\"recall\"]))\n",
    "    print(\"em:\", stat_sig_mcnemar(rag_baseline[\"em\"], var[\"em\"]))\n",
    "    print(\"f1:\", stat_sig_t_test(rag_baseline[\"f1\"], var[\"f1\"]))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
