{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_questions(chunk, model_name=\"llama3.2\", num_questions=3):\n",
    "    \"\"\"\n",
    "    Generate questions from a given text chunk using an Ollama model.\n",
    "    \n",
    "    Args:\n",
    "        chunk (str): The text chunk to generate questions from\n",
    "        model_name (str): The name of the Ollama model to use (default: \"llama3.2\")\n",
    "        num_questions (int): Number of questions to generate (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated questions\n",
    "    \"\"\"\n",
    "    # Ollama API endpoint (default is localhost on port 11434)\n",
    "    api_url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Create the prompt for question generation\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant that generates questions from text.\n",
    "    \n",
    "    Generate exactly {num_questions} relevant questions from the following text:\n",
    "    \n",
    "    {chunk}\n",
    "    \n",
    "    Format requirements:\n",
    "    1. Output exactly one question per line\n",
    "    2. Include ONLY the questions themselves\n",
    "    3. Do not include ANY numbering, bullets, prefixes, or explanatory text\n",
    "    4. Do not include phrases like \"Question:\" or \"Here are the questions:\"\n",
    "    5. Each line should be a complete, standalone question\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request to Ollama\n",
    "        response = requests.post(api_url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        return result[\"response\"]\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error communicating with Ollama: {e}\")\n",
    "        return None\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing Ollama response: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where were the Los Angeles Dodgers' World Series games played in 2020?\n"
     ]
    }
   ],
   "source": [
    "# chunk = \"The Los Angeles Dodgers won the World Series in 2020. The games were played in Arlington, Texas, at Globe Life Field due to the COVID-19 pandemic.\"\n",
    "\n",
    "# questions = generate_questions(chunk, model_name=\"llama3.2\", num_questions=1)\n",
    "# print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Where were the Los Angeles Dodgers' World Series games played in 2020?\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# questions.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of Pittsburgh's principal city in the greater Pittsburgh–Weirton–Steubenville combined statistical area? \n",
      "Why did Pittsburgh develop as a vital link between the Atlantic coast and Midwest?\n",
      "How many bridges does the city of Pittsburgh have?\n",
      "-------- ---------\n"
     ]
    }
   ],
   "source": [
    "## read csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"general_info.csv\", lineterminator='\\n')\n",
    "# print(df[\"text\"][0])\n",
    "for i in range(len(df)):\n",
    "    question = generate_questions(df[\"text\"][i], model_name=\"llama3.2\", num_questions=1)\n",
    "    print(question)\n",
    "    print(\"-------- ---------\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
